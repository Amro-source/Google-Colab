{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed forward Tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPLVWso2FJnCXecdzptjy3b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amro-source/Google-Colab/blob/main/Feed_forward_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wG0Z_lIUsdd",
        "outputId": "ef847541-ee01-4453-d181-847fa5be54e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "#tf.set_random_seed(RANDOM_SEED)\n",
        "#import tensorflow.compat.v1 as tf\n",
        "\n",
        "#tf.disable_v2_behavior()\n",
        "\n",
        "def init_weights(shape):\n",
        "    \"\"\" Weight initialization \"\"\"\n",
        "    weights = tf.random_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(weights)\n",
        "\n",
        "def forwardprop(X, w_1, w_2):\n",
        "    \"\"\"\n",
        "    Forward-propagation.\n",
        "    IMPORTANT: yhat is not softmax since TensorFlow's softmax_cross_entropy_with_logits() does that internally.\n",
        "    \"\"\"\n",
        "    h    = tf.nn.sigmoid(tf.matmul(X, w_1))  # The \\sigma function\n",
        "    yhat = tf.matmul(h, w_2)  # The \\varphi function\n",
        "    return yhat\n",
        "\n",
        "def get_iris_data():\n",
        "    \"\"\" Read the iris data set and split them into training and test sets \"\"\"\n",
        "    iris   = datasets.load_iris()\n",
        "    data   = iris[\"data\"]\n",
        "    target = iris[\"target\"]\n",
        "    \n",
        "    # Prepend the column of 1s for bias\n",
        "    N, M  = data.shape\n",
        "    all_X = np.ones((N, M + 1))\n",
        "    all_X[:, 1:] = data\n",
        "\n",
        "    # Convert into one-hot vectors\n",
        "    num_labels = len(np.unique(target))\n",
        "    all_Y = np.eye(num_labels)[target]  # One liner trick!\n",
        "    return train_test_split(all_X, all_Y, test_size=0.33, random_state=RANDOM_SEED)\n",
        "\n",
        "def main():\n",
        "    train_X, test_X, train_y, test_y = get_iris_data()\n",
        "    print(\"We are going to train a neural network\")\n",
        "    print(\"Be Patient\")\n",
        "\n",
        "    print (\"We need to work hard on our data\")\n",
        "    # Layer's sizes\n",
        "    x_size = train_X.shape[1]   # Number of input nodes: 4 features and 1 bias\n",
        "    #print(x_size,shape[1])\n",
        "    print(\"First we need to know X shape\")\n",
        "    print(train_X.shape[1]) \n",
        "    print(train_X.shape[0])\n",
        "    print(\"Then wE need to know Y Shape\")\n",
        "    print(train_y.shape[1])\n",
        "    print(train_y.shape[0])\n",
        "    print(train_X)\n",
        "    #print(train_y)\n",
        "\n",
        "    h_size = 256                # Number of hidden nodes\n",
        "    y_size = train_y.shape[1]   # Number of outcomes (3 iris flowers)\n",
        "    \n",
        "    # Symbols\n",
        "    X = tf.placeholder(\"float\", shape=[None, x_size])\n",
        "    \n",
        "    #X=tf.Variable(tf.ones(shape=[None, x_size]), dtype=tf.float32)\n",
        "\n",
        "    y = tf.placeholder(\"float\", shape=[None, y_size])\n",
        "\n",
        "    #y=tf.Variable(tf.ones(shape=[None, y_size]), dtype=tf.float32)\n",
        "\n",
        "    # Weight initializations\n",
        "    w_1 = init_weights((x_size, h_size))\n",
        "    w_2 = init_weights((h_size, y_size))\n",
        "\n",
        "    # Forward propagation\n",
        "    yhat    = forwardprop(X, w_1, w_2)\n",
        "    predict = tf.argmax(yhat, axis=1)\n",
        "\n",
        "    # Backward propagation\n",
        "    cost    = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=yhat))\n",
        "    updates = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
        "\n",
        "    # Run SGD\n",
        "    sess = tf.Session()\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "\n",
        "    for epoch in range(100):\n",
        "        # Train with each example\n",
        "        for i in range(len(train_X)):\n",
        "            sess.run(updates, feed_dict={X: train_X[i: i + 1], y: train_y[i: i + 1]})\n",
        "\n",
        "        train_accuracy = np.mean(np.argmax(train_y, axis=1) ==\n",
        "                                 sess.run(predict, feed_dict={X: train_X, y: train_y}))\n",
        "        test_accuracy  = np.mean(np.argmax(test_y, axis=1) ==\n",
        "                                 sess.run(predict, feed_dict={X: test_X, y: test_y}))\n",
        "\n",
        "        print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\"\n",
        "              % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
        "\n",
        "    sess.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are going to train a neural network\n",
            "Be Patient\n",
            "We need to work hard on our data\n",
            "First we need to know X shape\n",
            "5\n",
            "100\n",
            "Then wE need to know Y Shape\n",
            "3\n",
            "100\n",
            "[[1.  5.7 2.9 4.2 1.3]\n",
            " [1.  7.6 3.  6.6 2.1]\n",
            " [1.  5.6 3.  4.5 1.5]\n",
            " [1.  5.1 3.5 1.4 0.2]\n",
            " [1.  7.7 2.8 6.7 2. ]\n",
            " [1.  5.8 2.7 4.1 1. ]\n",
            " [1.  5.2 3.4 1.4 0.2]\n",
            " [1.  5.  3.5 1.3 0.3]\n",
            " [1.  5.1 3.8 1.9 0.4]\n",
            " [1.  5.  2.  3.5 1. ]\n",
            " [1.  6.3 2.7 4.9 1.8]\n",
            " [1.  4.8 3.4 1.9 0.2]\n",
            " [1.  5.  3.  1.6 0.2]\n",
            " [1.  5.1 3.3 1.7 0.5]\n",
            " [1.  5.6 2.7 4.2 1.3]\n",
            " [1.  5.1 3.4 1.5 0.2]\n",
            " [1.  5.7 3.  4.2 1.2]\n",
            " [1.  7.7 3.8 6.7 2.2]\n",
            " [1.  4.6 3.2 1.4 0.2]\n",
            " [1.  6.2 2.9 4.3 1.3]\n",
            " [1.  5.7 2.5 5.  2. ]\n",
            " [1.  5.5 4.2 1.4 0.2]\n",
            " [1.  6.  3.  4.8 1.8]\n",
            " [1.  5.8 2.7 5.1 1.9]\n",
            " [1.  6.  2.2 4.  1. ]\n",
            " [1.  5.4 3.  4.5 1.5]\n",
            " [1.  6.2 3.4 5.4 2.3]\n",
            " [1.  5.5 2.3 4.  1.3]\n",
            " [1.  5.4 3.9 1.7 0.4]\n",
            " [1.  5.  2.3 3.3 1. ]\n",
            " [1.  6.4 2.7 5.3 1.9]\n",
            " [1.  5.  3.3 1.4 0.2]\n",
            " [1.  5.  3.2 1.2 0.2]\n",
            " [1.  5.5 2.4 3.8 1.1]\n",
            " [1.  6.7 3.  5.  1.7]\n",
            " [1.  4.9 3.1 1.5 0.2]\n",
            " [1.  5.8 2.8 5.1 2.4]\n",
            " [1.  5.  3.4 1.5 0.2]\n",
            " [1.  5.  3.5 1.6 0.6]\n",
            " [1.  5.9 3.2 4.8 1.8]\n",
            " [1.  5.1 2.5 3.  1.1]\n",
            " [1.  6.9 3.2 5.7 2.3]\n",
            " [1.  6.  2.7 5.1 1.6]\n",
            " [1.  6.1 2.6 5.6 1.4]\n",
            " [1.  7.7 3.  6.1 2.3]\n",
            " [1.  5.5 2.5 4.  1.3]\n",
            " [1.  4.4 2.9 1.4 0.2]\n",
            " [1.  4.3 3.  1.1 0.1]\n",
            " [1.  6.  2.2 5.  1.5]\n",
            " [1.  7.2 3.2 6.  1.8]\n",
            " [1.  4.6 3.1 1.5 0.2]\n",
            " [1.  5.1 3.5 1.4 0.3]\n",
            " [1.  4.4 3.  1.3 0.2]\n",
            " [1.  6.3 2.5 4.9 1.5]\n",
            " [1.  6.3 3.4 5.6 2.4]\n",
            " [1.  4.6 3.4 1.4 0.3]\n",
            " [1.  6.8 3.  5.5 2.1]\n",
            " [1.  6.3 3.3 6.  2.5]\n",
            " [1.  4.7 3.2 1.3 0.2]\n",
            " [1.  6.1 2.9 4.7 1.4]\n",
            " [1.  6.5 2.8 4.6 1.5]\n",
            " [1.  6.2 2.8 4.8 1.8]\n",
            " [1.  7.  3.2 4.7 1.4]\n",
            " [1.  6.4 3.2 5.3 2.3]\n",
            " [1.  5.1 3.8 1.6 0.2]\n",
            " [1.  6.9 3.1 5.4 2.1]\n",
            " [1.  5.9 3.  4.2 1.5]\n",
            " [1.  6.5 3.  5.2 2. ]\n",
            " [1.  5.7 2.6 3.5 1. ]\n",
            " [1.  5.2 2.7 3.9 1.4]\n",
            " [1.  6.1 3.  4.6 1.4]\n",
            " [1.  4.5 2.3 1.3 0.3]\n",
            " [1.  6.6 2.9 4.6 1.3]\n",
            " [1.  5.5 2.6 4.4 1.2]\n",
            " [1.  5.3 3.7 1.5 0.2]\n",
            " [1.  5.6 3.  4.1 1.3]\n",
            " [1.  7.3 2.9 6.3 1.8]\n",
            " [1.  6.7 3.3 5.7 2.1]\n",
            " [1.  5.1 3.7 1.5 0.4]\n",
            " [1.  4.9 2.4 3.3 1. ]\n",
            " [1.  6.7 3.3 5.7 2.5]\n",
            " [1.  7.2 3.  5.8 1.6]\n",
            " [1.  4.9 3.6 1.4 0.1]\n",
            " [1.  6.7 3.1 5.6 2.4]\n",
            " [1.  4.9 3.  1.4 0.2]\n",
            " [1.  6.9 3.1 4.9 1.5]\n",
            " [1.  7.4 2.8 6.1 1.9]\n",
            " [1.  6.3 2.9 5.6 1.8]\n",
            " [1.  5.7 2.8 4.1 1.3]\n",
            " [1.  6.5 3.  5.5 1.8]\n",
            " [1.  6.3 2.3 4.4 1.3]\n",
            " [1.  6.4 2.9 4.3 1.3]\n",
            " [1.  5.6 2.8 4.9 2. ]\n",
            " [1.  5.9 3.  5.1 1.8]\n",
            " [1.  5.4 3.4 1.7 0.2]\n",
            " [1.  6.1 2.8 4.  1.3]\n",
            " [1.  4.9 2.5 4.5 1.7]\n",
            " [1.  5.8 4.  1.2 0.2]\n",
            " [1.  5.8 2.6 4.  1.2]\n",
            " [1.  7.1 3.  5.9 2.1]]\n",
            "Epoch = 1, train accuracy = 39.00%, test accuracy = 38.00%\n",
            "Epoch = 2, train accuracy = 65.00%, test accuracy = 70.00%\n",
            "Epoch = 3, train accuracy = 65.00%, test accuracy = 70.00%\n",
            "Epoch = 4, train accuracy = 66.00%, test accuracy = 70.00%\n",
            "Epoch = 5, train accuracy = 70.00%, test accuracy = 70.00%\n",
            "Epoch = 6, train accuracy = 70.00%, test accuracy = 74.00%\n",
            "Epoch = 7, train accuracy = 71.00%, test accuracy = 76.00%\n",
            "Epoch = 8, train accuracy = 76.00%, test accuracy = 78.00%\n",
            "Epoch = 9, train accuracy = 81.00%, test accuracy = 78.00%\n",
            "Epoch = 10, train accuracy = 83.00%, test accuracy = 78.00%\n",
            "Epoch = 11, train accuracy = 83.00%, test accuracy = 78.00%\n",
            "Epoch = 12, train accuracy = 87.00%, test accuracy = 78.00%\n",
            "Epoch = 13, train accuracy = 87.00%, test accuracy = 78.00%\n",
            "Epoch = 14, train accuracy = 87.00%, test accuracy = 78.00%\n",
            "Epoch = 15, train accuracy = 87.00%, test accuracy = 82.00%\n",
            "Epoch = 16, train accuracy = 88.00%, test accuracy = 84.00%\n",
            "Epoch = 17, train accuracy = 88.00%, test accuracy = 84.00%\n",
            "Epoch = 18, train accuracy = 88.00%, test accuracy = 84.00%\n",
            "Epoch = 19, train accuracy = 88.00%, test accuracy = 86.00%\n",
            "Epoch = 20, train accuracy = 89.00%, test accuracy = 86.00%\n",
            "Epoch = 21, train accuracy = 89.00%, test accuracy = 88.00%\n",
            "Epoch = 22, train accuracy = 89.00%, test accuracy = 88.00%\n",
            "Epoch = 23, train accuracy = 90.00%, test accuracy = 88.00%\n",
            "Epoch = 24, train accuracy = 92.00%, test accuracy = 90.00%\n",
            "Epoch = 25, train accuracy = 93.00%, test accuracy = 92.00%\n",
            "Epoch = 26, train accuracy = 93.00%, test accuracy = 92.00%\n",
            "Epoch = 27, train accuracy = 93.00%, test accuracy = 92.00%\n",
            "Epoch = 28, train accuracy = 93.00%, test accuracy = 92.00%\n",
            "Epoch = 29, train accuracy = 93.00%, test accuracy = 92.00%\n",
            "Epoch = 30, train accuracy = 93.00%, test accuracy = 92.00%\n",
            "Epoch = 31, train accuracy = 93.00%, test accuracy = 94.00%\n",
            "Epoch = 32, train accuracy = 94.00%, test accuracy = 94.00%\n",
            "Epoch = 33, train accuracy = 94.00%, test accuracy = 94.00%\n",
            "Epoch = 34, train accuracy = 94.00%, test accuracy = 94.00%\n",
            "Epoch = 35, train accuracy = 94.00%, test accuracy = 94.00%\n",
            "Epoch = 36, train accuracy = 94.00%, test accuracy = 94.00%\n",
            "Epoch = 37, train accuracy = 94.00%, test accuracy = 94.00%\n",
            "Epoch = 38, train accuracy = 94.00%, test accuracy = 96.00%\n",
            "Epoch = 39, train accuracy = 94.00%, test accuracy = 96.00%\n",
            "Epoch = 40, train accuracy = 94.00%, test accuracy = 96.00%\n",
            "Epoch = 41, train accuracy = 94.00%, test accuracy = 96.00%\n",
            "Epoch = 42, train accuracy = 95.00%, test accuracy = 98.00%\n",
            "Epoch = 43, train accuracy = 95.00%, test accuracy = 98.00%\n",
            "Epoch = 44, train accuracy = 95.00%, test accuracy = 98.00%\n",
            "Epoch = 45, train accuracy = 95.00%, test accuracy = 98.00%\n",
            "Epoch = 46, train accuracy = 95.00%, test accuracy = 98.00%\n",
            "Epoch = 47, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 48, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 49, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 50, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 51, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 52, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 53, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 54, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 55, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 56, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 57, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 58, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 59, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 60, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 61, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 62, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 63, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 64, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 65, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 66, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 67, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 68, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 69, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 70, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 71, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 72, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 73, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 74, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 75, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 76, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 77, train accuracy = 96.00%, test accuracy = 98.00%\n",
            "Epoch = 78, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 79, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 80, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 81, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 82, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 83, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 84, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 85, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 86, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 87, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 88, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 89, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 90, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 91, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 92, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 93, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 94, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 95, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 96, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 97, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 98, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 99, train accuracy = 96.00%, test accuracy = 100.00%\n",
            "Epoch = 100, train accuracy = 96.00%, test accuracy = 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMO97xrvVSKn"
      },
      "source": [
        "pip install tensorflow==1.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbv8CjpdZKA6"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# first, create a TensorFlow constant\n",
        "const = tf.constant(2.0, name=\"const\")\n",
        "    \n",
        "# create TensorFlow variables\n",
        "b = tf.Variable(2.0, name='b')\n",
        "c = tf.Variable(1.0, name='c')\n",
        "\n",
        "print(b)\n",
        "print(c)\n",
        "\n",
        "# now create some operations\n",
        "d = tf.add(b, c, name='d')\n",
        "e = tf.add(c, const, name='e')\n",
        "a = tf.multiply(d, e, name='a')\n",
        "\n",
        "\n",
        "# setup the variable initialisation\n",
        "#init_op = tf.global_variables_initializer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfsB9KLBYvrP"
      },
      "source": [
        "!pip install tensorflow==1.12.0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCkHd30MZB6_",
        "outputId": "3cafeab2-f146-4e16-d657-8feb7daeef19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}